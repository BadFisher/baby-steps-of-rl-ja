強化学習の弱点として以下3点を中心に解説
- サンプル効率が悪い
- 局所的な行動に陥る、過学習することが多い
- 再現性が低い

5章: 対処療法的な解決方法
- 弱点があることを前提とした上で影響を軽減するためのアプローチ
6章: 根本的な解決方法
- 弱点そのものの解決を目指したアプローチ

## サンプル効率が悪い

### 深層強化学習は学習に大量のサンプルを必要とする
Rainbowの論文
- Rainbowであっても、Atariのゲームで人間同等のスコアを記録するのに、30fpsの場合約166時間、60fpsの場合約80時間以上のプレイ時間がかかる

行動が連続値の場合はさらに難しくなる(Continuous Control)
- 例: 連続値コントーロールタスクを集めたdm_controlという環境において、D4PGの学習が収束するまでに4000万ステップ必要

大量のサンプルを用意する必要 → 何度もプレイ可能なシミュレータを用意する必要

ロボットなど物理世界のエージェントへの適用を難しくする
- 物理世界で数千万回ものプレイをさせることは困難
- ロボット開発で有名なBoston Dynamicsですら制御タスクには従来より使われている手法を用いている

### 個々の問題に特化した従来の手法 vs 深層強化学習
従来手法
- 深層強化学習より早く、安定的に学習できる

深層強化学習
- 汎用的であるがゆえに特化していない(器用貧乏)


## 局所最適な行動に陥る、過学習することが多い
強化学習
- 教師なし学習なので、人間が意図した行動を獲得してくれる保証がない
    - 想像を超えるよい行動を学習する可能性と好ましくない結果になる可能性の両方がある

エージェント陥ってしまうパターン
- 局所最適な行動
- 過学習

### 局所最適
報酬は獲得できているものの最適とは言えない行動
- 例: テストでがんばればもっと点がとれるのに平均点がとれて満足してしまう

落下してくるボールをキャッチするタスク
- たまたまボールが端にくることが多かった場合
- → 端の方でずっと待機して、端に落ちてきたボールを確実にキャッチできるよう学習

いったん局所解に陥ると抜け出すのが難しい
- 過学習から抜け出すには、「最適だと思っている行動は最適ではない」と確認させる新しい経験が必要
- ε-greedy法でεを下げていくような方策をとる場合、学習が進むごとにランダムに探索する機会が減るので、新しい経験を得る可能性が下がる

### 過学習
環境に特化した行動を獲得してしまうこと
- 例: テストの答案の丸暗記

対戦ゲームの場合、特定の対戦相手にだけ勝てる方法を学習してしまう

### 報酬設計がカギ

報酬設計があまい例
- レースコース上にアイテムが落ちているボートレースゲーム
    - 「速くゴールしたい」「できるだけアイテムをとって高いスコアをとりたい」
    - 報酬: スコア
- → 逆走してまでアイテムを取りにいくよう学習してしまった


## 再現性が低い
  * ハイパーパラメーターの設定はもちろん、実行のたびに結果が変わるようなケースがある。
* 対策
  * 根本的な対策は[Day6](https://github.com/icoxfog417/baby-steps-of-rl-ja#day6-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%AE%E5%BC%B1%E7%82%B9%E3%82%92%E5%85%8B%E6%9C%8D%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E6%89%8B%E6%B3%95)にて紹介し、Day5ではこの弱点を前提とした対策を紹介する。
  * 対策の基本は、「１回の学習結果を無駄にしない」となる。
  * 「再現性が低い」ため複数回の実験が必要になる。
  * しかし、「サンプル効率が悪い」ため学習には多くの時間が必要となる。
  * そのため、一回の実験は長時間X複数回の実行からなる。これには当然時間がかかる。
  * 時間のかかる実験がつまらないミスでやり直しになる事態を、可能な限り避ける必要がある。
  * また、一回の実験からは可能な限り情報を取りたい。
  * これを実現するため、Day4以降の実装では「モジュール分割」と「ログ取得」の2つを行っている。


## 局所的な行動に陥る、過学習することが多い
## 再現性が低い
