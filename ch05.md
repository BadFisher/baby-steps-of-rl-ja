# Pythonで学ぶ強化学習 Day5 まとめ

強化学習の弱点として以下3点を中心に解説
- サンプル効率が悪い
- 局所的な行動に陥る、過学習することが多い
- 再現性が低い

Day5: 対処療法的な解決方法
- 弱点があることを前提とした上で影響を軽減するためのアプローチ  

Day6: 根本的な解決方法
- 弱点そのものの解決を目指したアプローチ

## サンプル効率が悪い

### 深層強化学習は学習に大量のサンプルを必要とする
Rainbowの論文
- Rainbowであっても、Atariのゲームで人間同等のスコアを記録するのに、30fpsの場合約166時間、60fpsの場合約80時間以上のプレイ時間がかかる

行動が連続値の場合はさらに難しくなる(Continuous Control)
- 例: 連続値コントーロールタスクを集めたdm_controlという環境において、D4PGの学習が収束するまでに4000万ステップ必要

大量のサンプルを用意する必要 → 何度もプレイ可能なシミュレータを用意する必要

ロボットなど物理世界のエージェントへの適用を難しくする
- 物理世界で数千万回ものプレイをさせることは困難
- ロボット開発で有名なBoston Dynamicsですら制御タスクには従来より使われている手法を用いている

### 個々の問題に特化した従来の手法 vs 深層強化学習
従来手法
- 深層強化学習より早く、安定的に学習できる

深層強化学習
- 汎用的であるがゆえに特化していない(器用貧乏)


## 局所最適な行動に陥る、過学習することが多い
強化学習
- 教師なし学習なので、人間が意図した行動を獲得してくれる保証がない
    - 想像を超えるよい行動を学習する可能性と好ましくない結果になる可能性の両方がある

エージェント陥ってしまうパターン
- 局所最適な行動
- 過学習

### 局所最適
報酬は獲得できているものの最適とは言えない行動
- 例: テストでがんばればもっと点がとれるのに平均点がとれて満足してしまう

落下してくるボールをキャッチするタスク
- たまたまボールが端にくることが多かった場合
- → 端の方でずっと待機して、端に落ちてきたボールを確実にキャッチできるよう学習

いったん局所解に陥ると抜け出すのが難しい
- 過学習から抜け出すには、「最適だと思っている行動は最適ではない」と確認させる新しい経験が必要
- ε-greedy法でεを下げていくような方策をとる場合、学習が進むごとにランダムに探索する機会が減るので、新しい経験を得る可能性が下がる

### 過学習
環境に特化した行動を獲得してしまうこと
- 例: テストの答案の丸暗記

対戦ゲームの場合、特定の対戦相手にだけ勝てる方法を学習してしまう

### 報酬設計がカギ

報酬設計があまい例
- レースコース上にアイテムが落ちているボートレースゲーム
    - 「速くゴールしたい」「できるだけアイテムをとって高いスコアをとりたい」
    - 報酬: スコア
- → 逆走してまでアイテムを取りにいくよう学習してしまった


## 再現性が低い

実行するたび結果が変わるケース
- 同じアルゴリズム、同じパラメータで学習させたとしても、有意差が出るレベルで獲得報酬が異なる場合がある

実装の方法により結果に差が出るケース
- 各フレームワーク(Chainer, Keras, TensorFlowなど)のハイパーパラメータや重みのデフォルト値によって学習結果が変わることもある

サンプル効率が悪く、一度に学習に多くの時間がかかるため、結果の確認にも時間がかかる

## 弱点を前提とした対応策

可能な限り事前に入念な準備をした上で実験を行い、可能な限り取得したログから最善の対策を検討・実装した上で再度テスト・実装を行うというプロセスを繰り返すことが大事
- 対策の基本は、「１回の学習結果を無駄にしない」となる
    - 「再現性が低い」ため複数回の実験が必要
    - 「サンプル効率が悪い」ため学習には多くの時間が必要

### テスト可能なモジュールに切り分ける

時間のかかる実験がつまらないミスでやり直しになる事態を避けるため

Day4の例
- Observer, Trainer, Agent, Loggerモジュールに切り分けた

OpenAIの例
- 「Observerによる前処理の実装のミスで、敵キャラクターが消えてしまう」という問題が起きた
    - 前処理をした画面を実際に目で確かめられるようにした

Trainerを独立させることで、異なるエージェントを同じ学習方法で学習させることが可能
- ネットワーク構成が単縦なテスト用エージェントを用意し、パラメータなどを確認をすることができる

### 可能な限りログをとる

一回の実験から多くの知見を得るため
- 報酬の平均、最大、最小
    - 報酬を獲得する行動をとることがあるのかを確認
- エピソードの長さ
    - エージェントの生存期間の確認
- 目的関数の値、ネットワークの出力値
- 戦略から出力される行動分布のエントロピー
    - 過学習の傾向の確認

### 学習の自動化
パラメータを含めてスクリプト化しておいてGitで管理
- どういうパラメータ設定にしたか忘れないように
- 夜間に実行することで集中力が散漫にならないように

## Webビジネスにおける強化学習活用の現状

### Webサービスへの強化学習の活用はまだまだ進んでいない

強化学習は大量のデータが必要
- データはAgentが自ら収集するので、探索時に試した行動によってはデータが偏る
- → 整備された訓練データを利用するより学習が不安定になる

Webビジネスの分野はシミュレーション環境の開発が困難
- 膨大な施行数をこなす必要がある上に、現実世界では探索による危険性が伴う
    - 自動運転: 事故、Webビジネス: 収益減
- →Webビジネス以外の事例ではPC上で大量のシミュレーション高速で行い学習
- →Webビジネスでは報酬を決める要因が人の心理なのでシミュレートしにくい

強化学習は学習モデルに汎用性がない
他タスクへの転移学習の手法が確立されていないので、タスクが少しでも変わると学習し直し
- → 汎用学習済みモデルがない

Webビジネスの分野はタスクが「状態が遷移する問題設定」として扱いづらい

### タスクが「状態が遷移する問題設定」として扱いづらい
「未来の状態、報酬を考慮した行動が取れるようになる」という強化学習のメリットがうまく享受できない


